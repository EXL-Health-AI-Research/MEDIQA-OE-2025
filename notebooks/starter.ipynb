{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96ffd080",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8478a18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../src/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7490abb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from load_dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "971bc2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhinand/dev/work-research/MEDIQA-OE-2025/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from pprint import pprint\n",
    "from IPython.display import Markdown\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset, load_dataset\n",
    "\n",
    "from mediqa_oe.data import MedicalOrderDataLoader\n",
    "from mediqa_oe.lm import OrderExtractionLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d551a5",
   "metadata": {},
   "source": [
    "## Load Data and LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0271f8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = MedicalOrderDataLoader(trs_json_path=\"../data/orders_data_transcript.json\")\n",
    "\n",
    "ds, ds_val = data_loader.ds, data_loader.ds_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9395ef05",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = OrderExtractionLM(\n",
    "    backend=\"openai\",\n",
    "    model_name_or_path=\"\",\n",
    "    api_base=os.getenv(\"OPENAI_API_BASE\"),\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab72c87",
   "metadata": {},
   "source": [
    "## Methods Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b3ab3369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['google/medgemma-27b-text-it']\n"
     ]
    }
   ],
   "source": [
    "print([model.id for model in lm.impl.client.models.list().data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eaeb973d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Remote: https://e307wui0v6xrqf-8000.proxy.runpod.net/v1/'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.get_device_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6ad907c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I am a medical AI assistant designed to provide information and answer questions related to health and medicine in a single sentence.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_msg = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a medical AI assistant how answers in one sentence.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Hi, what kind of assistant are you?\",\n",
    "    },\n",
    "]\n",
    "\n",
    "out = lm.infer(\n",
    "    messages=test_msg\n",
    ")\n",
    "\n",
    "Markdown(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85a546a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am a medical AI assistant designed to provide information and answer questions related to health and medicine in a single sentence.\n"
     ]
    }
   ],
   "source": [
    "for chunk in lm.infer_stream(messages=test_msg):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ba78b0",
   "metadata": {},
   "source": [
    "## Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c8277ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are a medical AI assistant specialized in extracting medical orders from doctor-patient conversations.\n",
    "\n",
    "Your task is to identify and extract all medical orders mentioned by the doctor, including:\n",
    "1. Medications (prescriptions, dosage changes)\n",
    "2. Laboratory tests \n",
    "3. Imaging studies\n",
    "4. Follow-up appointments\n",
    "5. Referrals\n",
    "\n",
    "For each order, extract:\n",
    "- order_type: \"medication\", \"lab\", \"imaging\", \"followup\", or \"referral\"\n",
    "- description: Clear description of what is being ordered\n",
    "- reason: Medical condition or symptom being addressed\n",
    "- provenance: Turn numbers where this order is mentioned\n",
    "\n",
    "Return the results as a JSON list of objects.\"\"\"\n",
    "\n",
    "\n",
    "INSTRUCTION_TEMPLATE = \"\"\"Please extract all medical orders from the following doctor-patient conversation:\n",
    "\n",
    "CONVERSATION:\n",
    "{conversation}\n",
    "\n",
    "Extract all medical orders and return them as a JSON list with the following format:\n",
    "[\n",
    "  {{\n",
    "    \"order_type\": \"medication|lab|imaging|followup|referral\",\n",
    "    \"description\": \"specific description of the order\",\n",
    "    \"reason\": \"medical condition or reason for the order\", \n",
    "    \"provenance\": [list of turn numbers where this order appears]\n",
    "  }}\n",
    "]\n",
    "\n",
    "Focus on explicit orders given by the doctor. Be precise with medical terminology.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c359037",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _format_conv(turns, max_turns=-1, only_last_n=False):\n",
    "    formatted = []\n",
    "\n",
    "    if max_turns > 0:\n",
    "        turns = turns[-max_turns:] if only_last_n else turns[:max_turns]\n",
    "\n",
    "    for turn in turns:\n",
    "        speaker = turn['speaker']\n",
    "        text = turn['transcript']\n",
    "        turn_id = turn['turn_id']\n",
    "        formatted.append(f\"Turn {turn_id} - {speaker}: {text}\")\n",
    "    \n",
    "    return \"\\n\".join(formatted)\n",
    "\n",
    "\n",
    "def format_messages(conv):\n",
    "    instruction = INSTRUCTION_TEMPLATE.format(\n",
    "        conversation=conv,\n",
    "    )\n",
    "    instruction = f\"\"\"EXAMPLE CONVERSATION:\n",
    "Turn 126 - DOCTOR: so, for your first problem of your shortness of breath i think that you are in an acute heart failure exacerbation.\n",
    "Turn 127 - DOCTOR: i want to go ahead and, uh, put you on some lasix, 40 milligrams a day.\n",
    "Turn 138 - DOCTOR: for your second problem of your type i diabetes, um, let's go ahead... i wanna order a hemoglobin a1c for, um, uh, just in a, like a month or so.\n",
    "\n",
    "EXPECTED OUTPUT:\n",
    "[\n",
    "  {{\n",
    "    \"order_type\": \"medication\",\n",
    "    \"description\": \"lasix 40 milligrams a day\",\n",
    "    \"reason\": \"shortness of breath acute heart failure exacerbation\",\n",
    "    \"provenance\": [126, 127]\n",
    "  }},\n",
    "  {{\n",
    "    \"order_type\": \"lab\", \n",
    "    \"description\": \"hemoglobin a1c\",\n",
    "    \"reason\": \"type i diabetes\",\n",
    "    \"provenance\": [138]\n",
    "  }}\n",
    "]\n",
    "\n",
    "NOW EXTRACT FROM THIS CONVERSATION:\n",
    "\n",
    "---\n",
    "\n",
    "{instruction}\n",
    "\"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": SYSTEM_PROMPT,\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": instruction,\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    return messages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1279b99a",
   "metadata": {},
   "source": [
    "## Test with Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b28a5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = ds[1]\n",
    "\n",
    "sample_conv = _format_conv(sample_data[\"transcript\"])\n",
    "prompt = format_messages(conv=sample_conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cab434ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2081"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.token_count(prompt[-1]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "35a034e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "[\n",
      "  {\n",
      "    \"order_type\": \"lab\",\n",
      "    \"description\": \"pulmonary function test (pft)\",\n",
      "    \"reason\": \"check and baseline for lung function\",\n",
      "    \"provenance\": [27]\n",
      "  },\n",
      "  {\n",
      "    \"order_type\": \"imaging\",\n",
      "    \"description\": \"pet ct\",\n",
      "    \"reason\": \"determine if the lung nodule is metabolically active\",\n",
      "    \"provenance\": [27]\n",
      "  },\n",
      "  {\n",
      "    \"order_type\": \"followup\",\n",
      "    \"description\": \"continue to follow up with your rheumatologist\",\n",
      "    \"reason\": \"rheumatoid arthritis\",\n",
      "    \"provenance\": [27]\n",
      "  },\n",
      "  {\n",
      "    \"order_type\": \"medication\",\n",
      "    \"description\": \"continue your medication therapy\",\n",
      "    \"reason\": \"rheumatoid arthritis\",\n",
      "    \"provenance\": [27]\n",
      "  }\n",
      "]\n",
      "```"
     ]
    }
   ],
   "source": [
    "response = \"\"\n",
    "\n",
    "for chunk in lm.infer_stream(prompt):\n",
    "    response += chunk\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403460c2",
   "metadata": {},
   "source": [
    "## Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "36155cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_sample(sample, max_seqlen=8192):\n",
    "    sample[\"pred\"] = None\n",
    "    if not sample[\"transcript\"]:\n",
    "        print(f\"Transcript is None, skipping...\")\n",
    "        return sample\n",
    "    \n",
    "    sample_conv = _format_conv(sample[\"transcript\"])\n",
    "    prompt = format_messages(conv=sample_conv)\n",
    "\n",
    "    token_count = lm.token_count(prompt[-1]['content'])\n",
    "    if token_count > 0.9 * max_seqlen:\n",
    "        print(f\"Token length {token_count} exceeded max_seqlen {max_seqlen}, skipping...\")\n",
    "        return sample\n",
    "    \n",
    "    try:\n",
    "        out = lm.infer(messages=prompt, max_new_tokens=4096)\n",
    "        sample[\"pred\"] = out\n",
    "    except Exception as e:\n",
    "        print(f\"Error in LLM call -> {e}\")\n",
    "        return sample\n",
    "\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8f6cffd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4):   3%|▎         | 2/63 [00:17<08:23,  8.25s/ examples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript is None, skipping...\n",
      "Transcript is None, skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4):  14%|█▍        | 9/63 [00:32<02:09,  2.41s/ examples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript is None, skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4):  22%|██▏       | 14/63 [01:01<03:56,  4.83s/ examples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript is None, skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4):  25%|██▌       | 16/63 [01:13<04:09,  5.30s/ examples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript is None, skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4):  33%|███▎      | 21/63 [01:36<03:46,  5.39s/ examples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript is None, skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4):  40%|███▉      | 25/63 [01:40<01:53,  2.99s/ examples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript is None, skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4):  43%|████▎     | 27/63 [01:56<02:53,  4.82s/ examples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in LLM call -> Error code: 400 - {'object': 'error', 'message': \"This model's maximum context length is 8192 tokens. However, you requested 9341 tokens (5245 in the messages, 4096 in the completion). Please reduce the length of the messages or completion. None\", 'type': 'BadRequestError', 'param': None, 'code': 400}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4):  46%|████▌     | 29/63 [02:03<02:25,  4.29s/ examples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript is None, skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4):  54%|█████▍    | 34/63 [02:21<01:51,  3.86s/ examples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript is None, skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4):  73%|███████▎  | 46/63 [03:15<01:16,  4.49s/ examples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript is None, skipping...\n",
      "Error in LLM call -> Error code: 400 - {'object': 'error', 'message': \"This model's maximum context length is 8192 tokens. However, you requested 8434 tokens (4338 in the messages, 4096 in the completion). Please reduce the length of the messages or completion. None\", 'type': 'BadRequestError', 'param': None, 'code': 400}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4):  75%|███████▍  | 47/63 [03:16<00:55,  3.47s/ examples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript is None, skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4):  78%|███████▊  | 49/63 [03:31<01:08,  4.92s/ examples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript is None, skipping...\n",
      "Transcript is None, skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4):  89%|████████▉ | 56/63 [03:53<00:27,  3.88s/ examples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript is None, skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4): 100%|██████████| 63/63 [05:36<00:00,  5.34s/ examples]\n"
     ]
    }
   ],
   "source": [
    "ds = ds.map(infer_sample, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "07c8dc39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.filter(lambda x: x[\"transcript\"] is None).num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f00fcc80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.filter(lambda x: x[\"pred\"] is None).num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c464496d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 86.80ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "545220"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.to_json(\"../data/outputs/starter_outputs.jsonl\", ind)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
