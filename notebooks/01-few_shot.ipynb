{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96ffd080",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8478a18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../src/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7490abb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from load_dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971bc2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhinand/dev/work-research/MEDIQA-OE-2025/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from pprint import pprint\n",
    "from IPython.display import Markdown\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset, load_dataset\n",
    "\n",
    "from mediqa_oe.data import MedicalOrderDataLoader\n",
    "from mediqa_oe.lm import OrderExtractionLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d551a5",
   "metadata": {},
   "source": [
    "## Load Data and LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336e9f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_json_path = '<input_json_path for test>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0271f8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = MedicalOrderDataLoader(trs_json_path=input_json_path)\n",
    "\n",
    "ds, ds_val = data_loader.ds, data_loader.ds_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9395ef05",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = OrderExtractionLM(\n",
    "    backend=\"openai\",\n",
    "    model_name_or_path=\"\",\n",
    "    api_base=os.getenv(\"OPENAI_API_BASE\"),\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab72c87",
   "metadata": {},
   "source": [
    "## Methods Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3ab3369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['google/medgemma-27b-text-it']\n"
     ]
    }
   ],
   "source": [
    "print([model.id for model in lm.impl.client.models.list().data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaeb973d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.get_device_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6ad907c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I am a medical AI assistant designed to provide concise, one-sentence answers to health-related questions.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_msg = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a medical AI assistant how answers in one sentence.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Hi, what kind of assistant are you?\",\n",
    "    },\n",
    "]\n",
    "\n",
    "out = lm.infer(\n",
    "    messages=test_msg\n",
    ")\n",
    "\n",
    "Markdown(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c85a546a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am a medical AI assistant designed to provide information and answer questions related to health and medicine in a single sentence.\n"
     ]
    }
   ],
   "source": [
    "for chunk in lm.infer_stream(messages=test_msg):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ba78b0",
   "metadata": {},
   "source": [
    "## Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8277ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a medical AI assistant specialized in extracting EXPLICIT medical orders from doctor-patient conversations.\n",
    "\n",
    "CRITICAL RULES:\n",
    "1. Extract ONLY orders explicitly stated by the doctor\n",
    "2. Do NOT infer or assume orders that aren't clearly mentioned\n",
    "3. Provenance must be EXACT turn numbers where orders appear\n",
    "4. Be balanced - i.e precision and recall on level terms\n",
    "5. If the doctor orders multiple DISTINCT items (e.g., 'get a covid test and blood test'), create separate order objects for each item - never merge them into one combined description.\n",
    "\n",
    "Order Types:\n",
    "- medication: Prescriptions, dosage instructions, medication changes\n",
    "- lab: Blood tests, urine tests, specific diagnostic tests\n",
    "- imaging: X-rays, MRI, CT scans, ultrasounds\n",
    "- followup: Scheduled return visits, check-ups (these must be explicitly stated by the doctor)\n",
    "\n",
    "For each order extract:\n",
    "- order_type: One of the 4 types above\n",
    "- description: EXACT medical terminology used by doctor\n",
    "- reason: Specific condition/symptom mentioned by doctor\n",
    "- provenance: ONLY turn numbers where this exact order is mentioned\"\"\"\n",
    "\n",
    "\n",
    "INSTRUCTION_TEMPLATE = \"\"\"Please extract all medical orders from the following doctor-patient conversation:\n",
    "\n",
    "CONVERSATION:\n",
    "{conversation}\n",
    "\n",
    "Extract all medical orders and return them as a JSON list with the following format:\n",
    "[\n",
    "  {{\n",
    "    \"order_type\": \"medication|lab|imaging|followup|referral\",\n",
    "    \"description\": \"specific description of the order\",\n",
    "    \"reason\": \"medical condition or reason for the order\", \n",
    "    \"provenance\": [list of turn numbers where this order appears]\n",
    "  }}\n",
    "]\n",
    "\n",
    "Focus on explicit orders given by the doctor. Be precise with medical terminology.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c359037",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _format_conv(turns, max_turns=-1, only_last_n=False):\n",
    "    formatted = []\n",
    "\n",
    "    if max_turns > 0:\n",
    "        turns = turns[-max_turns:] if only_last_n else turns[:max_turns]\n",
    "\n",
    "    for turn in turns:\n",
    "        speaker = turn['speaker']\n",
    "        text = turn['transcript']\n",
    "        turn_id = turn['turn_id']\n",
    "        formatted.append(f\"Turn {turn_id} - {speaker}: {text}\")\n",
    "    \n",
    "    return \"\\n\".join(formatted)\n",
    "\n",
    "\n",
    "def format_messages(conv):\n",
    "    instruction = INSTRUCTION_TEMPLATE.format(\n",
    "        conversation=conv,\n",
    "    )\n",
    "    instruction = f\"\"\"EXAMPLE CONVERSATION:\n",
    "Turn 126 - DOCTOR: so, for your first problem of your shortness of breath i think that you are in an acute heart failure exacerbation.\n",
    "Turn 127 - DOCTOR: i want to go ahead and, uh, put you on some lasix, 40 milligrams a day.\n",
    "Turn 138 - DOCTOR: for your second problem of your type i diabetes, um, let's go ahead... i wanna order a hemoglobin a1c for, um, uh, just in a, like a month or so.\n",
    "\n",
    "EXPECTED OUTPUT:\n",
    "[\n",
    "  {{\n",
    "    \"order_type\": \"medication\",\n",
    "    \"description\": \"lasix 40 milligrams a day\",\n",
    "    \"reason\": \"shortness of breath acute heart failure exacerbation\",\n",
    "    \"provenance\": [126, 127]\n",
    "  }},\n",
    "  {{\n",
    "    \"order_type\": \"lab\", \n",
    "    \"description\": \"hemoglobin a1c\",\n",
    "    \"reason\": \"type i diabetes\",\n",
    "    \"provenance\": [138]\n",
    "  }}\n",
    "]\n",
    "\n",
    "NOW EXTRACT FROM THIS CONVERSATION:\n",
    "\n",
    "---\n",
    "\n",
    "{instruction}\n",
    "\"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": SYSTEM_PROMPT,\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": instruction,\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    return messages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1279b99a",
   "metadata": {},
   "source": [
    "## Test with Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b28a5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = ds[1]\n",
    "\n",
    "sample_conv = _format_conv(sample_data[\"transcript\"])\n",
    "prompt = format_messages(conv=sample_conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cab434ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2081"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.token_count(prompt[-1]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35a034e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "[\n",
      "  {\n",
      "    \"order_type\": \"lab\",\n",
      "    \"description\": \"pulmonary function test (pft)\",\n",
      "    \"reason\": \"check and baseline for lung function\",\n",
      "    \"provenance\": [27]\n",
      "  },\n",
      "  {\n",
      "    \"order_type\": \"imaging\",\n",
      "    \"description\": \"pet ct\",\n",
      "    \"reason\": \"determine if the lung nodule is metabolically active\",\n",
      "    \"provenance\": [27]\n",
      "  },\n",
      "  {\n",
      "    \"order_type\": \"followup\",\n",
      "    \"description\": \"continue to follow up with your rheumatologist\",\n",
      "    \"reason\": \"rheumatoid arthritis\",\n",
      "    \"provenance\": [27]\n",
      "  },\n",
      "  {\n",
      "    \"order_type\": \"medication\",\n",
      "    \"description\": \"continue your medication therapy\",\n",
      "    \"reason\": \"rheumatoid arthritis\",\n",
      "    \"provenance\": [27]\n",
      "  }\n",
      "]\n",
      "```"
     ]
    }
   ],
   "source": [
    "response = \"\"\n",
    "\n",
    "for chunk in lm.infer_stream(prompt):\n",
    "    response += chunk\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403460c2",
   "metadata": {},
   "source": [
    "## Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "36155cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_sample(sample, max_seqlen=8192):\n",
    "    sample[\"pred\"] = None\n",
    "    if not sample[\"transcript\"]:\n",
    "        print(f\"Transcript is None, skipping...\")\n",
    "        return sample\n",
    "    \n",
    "    sample_conv = _format_conv(sample[\"transcript\"])\n",
    "    prompt = format_messages(conv=sample_conv)\n",
    "\n",
    "    token_count = lm.token_count(prompt[-1]['content'])\n",
    "    if token_count > 0.9 * max_seqlen:\n",
    "        print(f\"Token length {token_count} exceeded max_seqlen {max_seqlen}, skipping...\")\n",
    "        return sample\n",
    "    \n",
    "    try:\n",
    "        out = lm.infer(messages=prompt, max_new_tokens=2048)\n",
    "        sample[\"pred\"] = out\n",
    "    except Exception as e:\n",
    "        print(f\"Error in LLM call -> {e}\")\n",
    "        return sample\n",
    "\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6cffd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.map(infer_sample, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c8dc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.filter(lambda x: x[\"transcript\"] is None).num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00fcc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.filter(lambda x: x[\"pred\"] is None).num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c464496d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.to_json(\"/medgemma_llm_out_train__init.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "526d4665",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4):   3%|▎         | 3/100 [00:20<07:56,  4.91s/ examples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript is None, skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4):   7%|▋         | 7/100 [00:35<05:19,  3.44s/ examples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript is None, skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4):  13%|█▎        | 13/100 [01:18<09:05,  6.27s/ examples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript is None, skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4):  15%|█▌        | 15/100 [01:23<06:03,  4.28s/ examples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript is None, skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4):  22%|██▏       | 22/100 [01:46<04:37,  3.55s/ examples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript is None, skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4):  23%|██▎       | 23/100 [01:48<03:53,  3.03s/ examples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript is None, skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4):  32%|███▏      | 32/100 [02:37<04:32,  4.01s/ examples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript is None, skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4):  36%|███▌      | 36/100 [03:14<08:41,  8.15s/ examples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript is None, skipping...\n",
      "Transcript is None, skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4):  39%|███▉      | 39/100 [03:23<05:33,  5.46s/ examples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript is None, skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4):  43%|████▎     | 43/100 [03:43<04:55,  5.19s/ examples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript is None, skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4):  48%|████▊     | 48/100 [03:48<02:10,  2.50s/ examples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript is None, skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4):  53%|█████▎    | 53/100 [04:20<03:33,  4.55s/ examples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript is None, skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4):  54%|█████▍    | 54/100 [04:22<03:10,  4.13s/ examples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript is None, skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4):  56%|█████▌    | 56/100 [04:28<02:39,  3.63s/ examples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript is None, skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4):  61%|██████    | 61/100 [04:57<03:27,  5.31s/ examples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript is None, skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4):  68%|██████▊   | 68/100 [05:24<02:20,  4.39s/ examples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript is None, skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4):  75%|███████▌  | 75/100 [06:02<01:56,  4.68s/ examples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript is None, skipping...\n",
      "Transcript is None, skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4):  77%|███████▋  | 77/100 [06:11<01:41,  4.43s/ examples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript is None, skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4):  86%|████████▌ | 86/100 [06:44<00:37,  2.67s/ examples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript is None, skipping...\n",
      "Transcript is None, skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4):  98%|█████████▊| 98/100 [08:31<00:33, 16.97s/ examples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript is None, skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4): 100%|██████████| 100/100 [08:52<00:00,  5.33s/ examples]\n"
     ]
    }
   ],
   "source": [
    "ds_val = ds_val.map(infer_sample, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699b9d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_val.filter(lambda x: x[\"transcript\"] is None).num_rows, ds_val.filter(lambda x: x[\"pred\"] is None).num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bdc6bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 41.97ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "944276"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_val.to_json(\"/outputs/medgemma_llm_out_val__init.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1222f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
